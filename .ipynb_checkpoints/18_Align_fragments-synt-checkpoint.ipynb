{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/tf_gpu2/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# RUN Main import block and TODO list\n",
    "\n",
    "# TODO: see how uri calculated the ridges\n",
    "\n",
    "# TODO: Perform Histogram equalization - start with it\n",
    "# TODO: \n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph \n",
    "# This will be the peakness measure for the PSD ==> The desired ridge index\n",
    "# TODO:\n",
    "# take integral from the Highest peak+-0.005 divide by integral of the entire graph - it's the peakness measure for the PSD\n",
    "# must select a peak above a min threshold in order to ignore noisy frequency\n",
    "# must ignore peaks above a certain threshold in order to detect meaningful frequency\n",
    "# run the PSD in moving windows every 200 px (deduced from the below PSD pointing to a freq of 1/0.02=50-> times 4= 200px)\n",
    "# and medianf the result of the windows\n",
    "# TODO:\n",
    "# Another alternative: (with Yariv)\n",
    "# Run PSD column by column - get the phase, freq, peakness and reconstruct an artificial ridge slice\n",
    "# from this - reconstruct a \"clean\" artificial ridge image\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from scipy import ndimage\n",
    "from scipy import signal\n",
    "#import cv2\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import mahotas as mh\n",
    "from mahotas import polygon\n",
    "# import pymorph as pm\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "from scipy import ndimage as nd\n",
    "import skimage.transform as transform\n",
    "import skimage.morphology as mp\n",
    "import skimage.io as sio\n",
    "import scipy.misc as sm\n",
    "from skimage.filters import threshold_otsu, threshold_adaptive\n",
    "from skimage.feature import hessian_matrix, hessian_matrix_eigvals\n",
    "from skimage import exposure\n",
    "from skimage import data, img_as_float\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from bisect import bisect_left\n",
    "import math\n",
    "import warnings\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "# Pandas is used for data manipulation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "X_START = 1000\n",
    "X_END = 6000\n",
    "Y_START = 800\n",
    "Y_END = 4300\n",
    "BG_2_OBJ_RATIO = 0.91\n",
    "CUBE_SIZE = 250\n",
    "EDGE_GAP = 50\n",
    "# ROOT_FOLDER = \"/home/il239838/files/\"\n",
    "ROOT_FOLDER = \"/Users/il239838/Downloads/private/Thesis/Papyrus/PX303/files/\"\n",
    "LEARNING_RATE = 0.001\n",
    "BATCHES = 800\n",
    "BATCH_SIZE = 50\n",
    "BREAK_VAL = 1000\n",
    "\n",
    "cube_size = 250\n",
    "HORIZ_TOLERANCE_FACTOR = 50\n",
    "VERT_TOLERANCE_FACTOR = 75\n",
    "EDGE_GAP = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# UTILS\n",
    "def renameFragmentsFiles(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_ in files:\n",
    "            split = file_.split(\"_\")\n",
    "            if len(split) > 1 and split[0] != '.DS':\n",
    "                fileSplit = split[0].split(\"-\")\n",
    "\n",
    "                # next line for testing !!!\n",
    "                newFileName = fileSplit[0] + fileSplit[1] + \"_\" + split[1] + \"_\" + split[2]\n",
    "\n",
    "                # next line for validation !!!\n",
    "                #newFileName = fileSplit[0] + fileSplit[1] + \".jpg\"\n",
    "\n",
    "                os.rename(os.path.join(root, file_), \n",
    "                          os.path.join(root, newFileName))\n",
    "    \n",
    "\n",
    "def handle_match_row(df, idx, df_row):\n",
    "    rectanglesArr = eval(df_row[\"fragmentAndSideDrawRect\"])\n",
    "    if len(rectanglesArr) > 0:\n",
    "        firstImg = Image.open(ROOT_FOLDER+\"fragments/\"+df_row[\"firstFileName\"]+\".jpg\")\n",
    "        secondImg = Image.open(ROOT_FOLDER+\"fragments/\"+df_row[\"secondFileName\"]+\".jpg\")\n",
    "        # import pdb; pdb.set_trace()\n",
    "        firstRotate = (df_row[\"fragmentAndSide\"][df_row[\"fragmentAndSide\"].rfind(\"P\")-2] == \"0\")\n",
    "        secondRotate = (df_row[\"fragmentAndSide\"][-1] == \"1\")\n",
    "\n",
    "        # rotate the images if needed\n",
    "        if firstRotate:\n",
    "            firstImg = firstImg.rotate(180)\n",
    "        if secondRotate:\n",
    "            secondImg = secondImg.rotate(180)\n",
    "\n",
    "        cubesArr = eval(df_row[\"fragmentAndSideCubes\"])\n",
    "        pointsArr = eval(df_row[\"fragmentAndSideMatchPoint\"])\n",
    "\n",
    "        # correct the cubes, rects and points incase we had a rotation\n",
    "        for cube, rect, point in zip(cubesArr, rectanglesArr, pointsArr):\n",
    "            if firstRotate:\n",
    "                cube[0] = firstImg.size[0] - cube[0] - cube_size # reduce cube_size cause we measure from top left corner of the cube\n",
    "                cube[1] = firstImg.size[1] - cube[1] - cube_size # reduce cube_size - see above\n",
    "                rect[0] = cube[0] + cube_size + EDGE_GAP - HORIZ_TOLERANCE_FACTOR\n",
    "                rect[2] = cube[0] + cube_size + EDGE_GAP + HORIZ_TOLERANCE_FACTOR\n",
    "                point[0] = cube[0] + cube_size + EDGE_GAP\n",
    "            if secondRotate:\n",
    "                cube[2] = secondImg.size[0] - cube[2] - cube_size # reduce cube_size - see above\n",
    "                cube[3] = secondImg.size[1] - cube[3] - cube_size # reduce cube_size - see above\n",
    "            if firstRotate or secondRotate:\n",
    "                rect[1] = cube[1] - cube[3] - VERT_TOLERANCE_FACTOR\n",
    "                rect[3] = cube[1] - cube[3] + VERT_TOLERANCE_FACTOR\n",
    "                point[1] = cube[1] - cube[3]\n",
    "        \n",
    "        # if we rotated - need to write the updated values to the enhanced output file\n",
    "        if firstRotate or secondRotate:\n",
    "            df.at[idx, \"rotateFragmentAndSideCubes\"] = cubesArr\n",
    "            df.at[idx, \"rotateFragmentAndSideDrawRect\"] = rectanglesArr\n",
    "            df.at[idx, \"rotateFragmentAndSideMatchPoint\"] = pointsArr # TEST this! - I changed it after it was working?\n",
    "        \n",
    "        # rectanglesArr contains the projections of the matched position as they are reflected by each pair\n",
    "        # In this block we set the size for the overlap slate\n",
    "        mins = np.amin(rectanglesArr, 0)\n",
    "        maxs = np.amax(rectanglesArr, 0)\n",
    "        minsmaxs = [mins[0], mins[1], maxs[2], maxs[3]]\n",
    "        minsmins = [mins[0], mins[1], mins[0], mins[1]]\n",
    "        width = maxs[2] - mins[0]\n",
    "        height = maxs[3] - mins[1]\n",
    "        slate = np.zeros((width, height))\n",
    "\n",
    "        # Now we iterate over the matched pair and add each match rectangle to the overlap count on the slate\n",
    "        for rect in rectanglesArr:\n",
    "            rect_slide = np.zeros((width, height))\n",
    "            rect_adjusted =  np.subtract(rect, minsmins)\n",
    "            rect_slide[rect_adjusted[0]:rect_adjusted[2], rect_adjusted[1]:rect_adjusted[3]] = 1\n",
    "            slate = slate + rect_slide\n",
    "\n",
    "        # Then we determine what's the max overlap that we observe and keep it and its size and derivative in the df\n",
    "        # so we will use it later for the 2nd phase classification algorithm\n",
    "        slate_max = np.amax(slate)\n",
    "        df.at[idx, \"votesOverlapMax\"] = slate_max\n",
    "        df.at[idx, \"divideOverlapMaxBySideTotal\"] = float(slate_max) / df_row[\"fragmentAndSideTotal\"]\n",
    "        max_indices = np.where(slate == slate_max)\n",
    "        df.at[idx, \"votesOverlapHeight\"] = (max_indices[1][-1]+1) - max_indices[1][0]\n",
    "    \n",
    "        # Now we create a binary mask from the overlap max projection\n",
    "        slate_mask = np.copy(slate)\n",
    "        slate_mask[slate_mask < slate_max] = 0\n",
    "        slate_mask[slate_mask == slate_max] = 1\n",
    "        slate_mask_size = float(len(np.where(slate_mask == 1)[0]))\n",
    "\n",
    "        # Calculate per each match pair what is the overlap of their projection with the mask of the max overlap\n",
    "        overlaps_percent_arr = []\n",
    "        for rect in rectanglesArr:\n",
    "            rect_slide = np.zeros((width, height))\n",
    "            rect_adjusted =  np.subtract(rect, minsmins)\n",
    "            rect_slide[rect_adjusted[0]:rect_adjusted[2], rect_adjusted[1]:rect_adjusted[3]] = 1\n",
    "            overlap_indices = np.where((rect_slide == slate_mask) & (rect_slide == 1))\n",
    "            overlaps_percent_arr.append(float(len(overlap_indices[0]))/slate_mask_size)\n",
    "\n",
    "        # We'll now use the mid-point of the max-overlap rectangle (based on the saved indices) in order to \n",
    "        # place and align the fragments\n",
    "        # FIXME: need to fix next line as the square might be a jigsaw - need to pick the overall min and max indices\n",
    "        voted_square = [max_indices[0][0], max_indices[1][0], max_indices[0][-1]+1, max_indices[1][-1]+1]\n",
    "        adjusted_vote = np.add(minsmins,voted_square)\n",
    "        mid_point = [(adjusted_vote[0] + adjusted_vote[2])/2, (adjusted_vote[1] + adjusted_vote[3])/2]\n",
    "        \n",
    "        # We prepare the measurements of the connected image\n",
    "        con_width = firstImg.size[0] + secondImg.size[0]\n",
    "        con_height = 0\n",
    "        first_offset = (-adjusted_vote[1]) if adjusted_vote[1] < 0 else 0\n",
    "        second_offset = int(mid_point[1] + first_offset)\n",
    "        adjusted_vote[1] += first_offset\n",
    "        adjusted_vote[3] += first_offset\n",
    "        con_height = int(np.maximum(firstImg.size[1] + first_offset, secondImg.size[1] + second_offset))\n",
    "\n",
    "        # We paste the 2 images into the connected image in the correct offsets\n",
    "        conImage = Image.new('RGBA', (con_width, con_height))\n",
    "        conImage.paste(firstImg, (0, first_offset))\n",
    "        conImage.paste(secondImg, (firstImg.size[0]+1, second_offset))\n",
    "\n",
    "        # Draw the image and draw the max overlap rectangle\n",
    "        draw = ImageDraw.Draw(conImage)\n",
    "        draw.rectangle(adjusted_vote.tolist(), fill=\"green\", outline=\"green\")\n",
    "\n",
    "        # Draw the lines from one image to the other based on the cubes\n",
    "        cubeMid = CUBE_SIZE / 2\n",
    "        for overlap_percent, cube_pair in zip(overlaps_percent_arr, cubesArr):\n",
    "            # import pdb; pdb.set_trace()\n",
    "            color=\"\"\n",
    "            lwidth=1\n",
    "            if (overlap_percent < 0.1):\n",
    "                color=\"red\"\n",
    "            elif (overlap_percent < 0.5):\n",
    "                color=\"yellow\"\n",
    "                lwidth=3\n",
    "            else:\n",
    "                color=\"green\"\n",
    "                lwidth=5\n",
    "                df.at[idx, \"votesSupportOverlapRect\"] += 1 # if the matched pair overlaps the rectangle more than 50% - we consider this to be supporting\n",
    "            draw.line((cube_pair[0] + cubeMid, cube_pair[1] + cubeMid + first_offset,\n",
    "                      firstImg.size[0] + 1 + cube_pair[2] + cubeMid, cube_pair[3] + cubeMid + second_offset), \n",
    "                      fill=color, width=lwidth)\n",
    "\n",
    "        # Save derivatives of supporting cubes which match the max overlap\n",
    "        df.at[idx, \"divideSupportOverlapBySideTotal\"] = \\\n",
    "            float(df.at[idx, \"votesSupportOverlapRect\"]) / df_row[\"fragmentAndSideTotal\"]\n",
    "        df.at[idx, \"divideSupportOverlapBySideVote\"] = \\\n",
    "            float(float(df.at[idx, \"votesSupportOverlapRect\"])) / df_row[\"fragmentAndSideVote\"]\n",
    "\n",
    "        conImage.save(ROOT_FOLDER+\"squares/\"+str(df_row[\"class\"])+\"=\"+df_row[\"fragmentAndSide\"]+\".jpg\")\n",
    "    \n",
    "    \n",
    "def draw_histogram_for_row(df_row, bins):\n",
    "    matchPointArr = eval(df_row[\"fragmentAndSideMatchPoint\"])\n",
    "    vals = [row[1] for row in matchPointArr]\n",
    "    plt.hist(vals, bins, facecolor='green')\n",
    "    plt.title(df_row[\"fragment\"])\n",
    "    plt.show()\n",
    "    # import pdb; pdb.set_trace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# RUN ONCE ONLY!!!\n",
    "renameFragmentsFiles(ROOT_FOLDER+\"fragments/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# EITHER RUN THIS: CREATE CUBES WITH VOTES\n",
    "all_matches = pd.read_csv('20181216_034921_votes_cubes_match_synt.csv')\n",
    "# all_matches = all_matches[all_matches[\"class\"] == 1] # run only the classified or all?\n",
    "all_matches[\"votesOverlapMax\"] = 0\n",
    "all_matches[\"divideOverlapMaxBySideTotal\"] = 0.0\n",
    "all_matches[\"votesOverlapHeight\"] = 0\n",
    "all_matches[\"votesSupportOverlapRect\"] = 0\n",
    "all_matches[\"divideSupportOverlapBySideTotal\"] = 0.0\n",
    "all_matches[\"divideSupportOverlapBySideVote\"] = 0.0\n",
    "all_matches[\"rotateFragmentAndSideCubes\"] = all_matches[\"fragmentAndSideCubes\"] # check these!!\n",
    "all_matches[\"rotateFragmentAndSideDrawRect\"] = all_matches[\"fragmentAndSideDrawRect\"]\n",
    "all_matches[\"rotateFragmentAndSideMatchPoint\"] = all_matches[\"fragmentAndSideMatchPoint\"]\n",
    "for idx, row in all_matches.iterrows():\n",
    "    #if row[\"fragment\"] == \"PX303Fg006_7X5_5X2_PX303Fg006_7X5_6X2\":\n",
    "    # print(idx)\n",
    "    handle_match_row(all_matches, idx, row)\n",
    "\n",
    "all_matches.to_csv('20181216_034921_pairs_final_enhanced_synt.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OR RUN THIS: CREATE HISTOGRAM\n",
    "\n",
    "all_matches = pd.read_csv('votes_cubes_match_synt.csv') #('real_cubes_all_vote.csv')\n",
    "all_matches = all_matches[all_matches[\"class\"] == 1]\n",
    "for idx, row in all_matches.iterrows():\n",
    "    draw_histogram_for_row(row, 15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
